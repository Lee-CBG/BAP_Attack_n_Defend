name: ppo

configs:
  learning_rate: 3e-5  # higher learning rate for better exploration
  batch_size: 128  # Smaller batch size for more frequent updates
  mini_batch_size: 128
  # gradient_accumulation_steps: 1
  ppo_epochs: 1  # More epochs to better refine the policy in each iteration
  adap_kl_ctrl: False  # Disable adaptive KL control
  init_kl_coef: 1.2  # Disable the KL divergence penalty
  # cliprange: 0.3  # increase cliprange to allow larger updates
  steps: 5

attackable_configs:
  learning_rate: 1e-5  # atmTCR attackable learning rate
  batch_size: 128  
  mini_batch_size: 128
  gradient_accumulation_steps: 1
  ppo_epochs: 8  
  adap_kl_ctrl: True  
  init_kl_coef: 1e-1  
  steps: 5

generation_kwargs:
  min_length: -1 # don't ignore the EOS token
  top_k: 0.0 # no top-k sampling
  top_p: 1.0 # no nucleus sampling
  do_sample: True # yes, we want to sample
  max_length: 72
  num_return_sequences: 1


comparison_kwargs:
  temperature: 1.2
  min_length: -1 # don't ignore the EOS token
  top_k: 0.0 # no top-k sampling
  top_p: 1.0 # no nucleus sampling
  do_sample: True # yes, we want to sample
  max_length: 72
  num_return_sequences: ${rlhf.K}